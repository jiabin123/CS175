{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# ---------------------------------------------------------------------------------------\n",
    "# CS 175, WINTER 2021: ASSIGNMENT 1\n",
    "#\n",
    "# The goal of this assignment is to give you some practice with manipulating text data, \n",
    "# including tokenization and creation of a sparse bag-of-words array for a set of documents,\n",
    "# as well as building a simple logistic classifier and looking at which words get large\n",
    "# positive and negative weights on a dataset of 20,000 reviews from Yelp\n",
    "# \n",
    "# You should install Anaconda with Python 3.7 or above before starting this assignment\n",
    "#\n",
    "# General notes\n",
    "#\t- do not remove any code that is already provided: add your own code where appropriate\n",
    "#\t- add comments inline in the text to explain what you are doing \n",
    "#\t- feel free to add error checking if you wish (but we will not grade you on this) \n",
    "#\t- when you are done submit a copy of your edited version of this file, as Assignment1.py\n",
    "#   - be sure to test your code on some simple examples before you submit it\n",
    "#\n",
    "# Grading\n",
    "#   - problems 1 through 6 are each worth 20 points\n",
    "#   - points will be deducted if \n",
    "# \t\t- the code does not execute \n",
    "#       - the code does not return the correct answers on simple test cases, \n",
    "#\t\t- if the code is not general and only works on special cases, \n",
    "#\t\t- if there are very few or no comments.\n",
    "# \n",
    "# Submission\n",
    "#\t- your edited copy of assignment1.py\n",
    "#\t- a text file called assignment1.txt with the output from run_assignment1.py\n",
    "#\t- submit both files to Canvas\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# NOTE: for this assignment you will need to import the following libraries/modules\n",
    "# All of these should be installed on your system if you have the latest version of Anaconda installed\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "import simplejson as jsoncm\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# PROBLEM 1\n",
    "# Complete the definition of the function below so that it can take in a string\n",
    "# and return the percentage of alphabetical characters in a string that match a particular\n",
    "# alphabetical letter, where matching is not case-sensitive. The percentage is computed\n",
    "# relative to the number of alphabetic characters in the string (so numbers, punctuation,\n",
    "# white space, and all other non-alphabetic characters are ignored).\n",
    "#\n",
    "# NOTE: please read Section 3 of Chapter 1 in the NLTK online book to understand how to use the FreqDist method \n",
    "# ---------------------------------------------------------------------------------------\n",
    "def letter_percentage(text,letter):\n",
    "\t\"\"\"\n",
    "\tParameters:\n",
    "\ttext: string\n",
    "\tletter: a single alphabetical character (in lower case, e.g., 'a', 'b', ...)\n",
    "\t\n",
    "\tReturns:\n",
    "\tpercentage: the percentage of alphabetical characters in <text> that match <letter>\n",
    "\t\t  where a match is defined irrespective of the case of the characters in <text>\n",
    "\t\t\t\t\n",
    "\tExample:\n",
    "\tletter_percentage('This is a cat.','t')  returns 20.0\n",
    "\t\"\"\"\n",
    "\n",
    "     # extract a list of alphabetic characters and convert to lower case\n",
    "\tcharlist = [alpha for alpha in text.lower() if alpha.isalnum()]\n",
    "\n",
    "     # create an fdist object for the list of lower case characters\n",
    "\tfdist = nltk.probability.FreqDist(charlist)\n",
    " \n",
    "     # calculate the frequency of the specific character \"letter\"\n",
    "\tfrequency = fdist[letter]/len(charlist)\n",
    " \n",
    "     # convert the frequency to a percentage\n",
    "\tcharacter_percent = 100*frequency\n",
    "\tp = '{0:.2f}'.format(character_percent)\n",
    "\tprint('\\nPercentage = ',p,' of characters match the character',letter)\n",
    "\treturn character_percent\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# PROBLEM 2\n",
    "# Complete the definition of the function below so that it can take as input either\n",
    "# (a) a string or (b) a list of tokens of type nltk.text.Text\n",
    "# convert the string to word tokens, run the NLTK part of speech parser on the word tokens\n",
    "# using the 'universal' tagset, print out to the screen the percentage of tokens in the\n",
    "# that correspond to each type of tag, and return a list of pairs of tokens and tags.\n",
    "# \n",
    "# NOTE:\n",
    "# \t- please read Section 1 of Chapter 5 in the NLTK online book for information about part of speech tagging\n",
    "# \t- for word tokenization use the NLTK word_tokenize function with default settings\n",
    "#  \t- Print out the tags in order of decreasing frequency of occurrence\n",
    "#\t- Percentages printed out should be formatted to 2 decimal places of precision\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def parts_of_speech(s):\n",
    "\t\"\"\"\n",
    "\tParameters:\n",
    "\ts: input text as a string \n",
    "\t\n",
    "\tReturns:\n",
    "\tThe list of tokens and their POS tags from the string s, as a list of sublists \n",
    "\tPrints out the total number of tokens and percentage of tokens with each tag \n",
    "\t\t\t\t\n",
    "\tExample:\n",
    "\ts = 'This is a sentence. And this is a second sentence! Cool.'\n",
    "\tz1, z2 = parts_of_speech(s)  \n",
    "\t\tTotal number of tokens is 14\n",
    "\t\tTag: DET           Percentage of tokens =  28.57\n",
    "\t\tTag: .             Percentage of tokens =  21.43\n",
    "\t\tTag: NOUN          Percentage of tokens =  21.43\n",
    "\t\t....\n",
    "\t\"\"\" \n",
    "\t \n",
    "\t# tokenize the string into word tokens\n",
    "\ttokens = word_tokenize(s)\n",
    "\t\n",
    "    # extract POS tags using the universal tagset with the NLTK POS tagger\n",
    "\ttokens_and_tags = nltk.pos_tag(tokens,tagset = 'universal')\n",
    "\t\t\n",
    "\t# Compute and print the total number of tokens  \n",
    "\tn = len(tokens)\n",
    "\tprint('Total number of tokens is',n)\n",
    "\t\n",
    "\t# extract the tags\n",
    "\ttags = [ item[1] for item in tokens_and_tags ]  \n",
    "\t\n",
    "\t# count how often each of the tags occurs using FreqDist (from NLTK)\n",
    "\ttag_counts = nltk.probability.FreqDist(tags)\n",
    " \n",
    "     # sort the tag counts by frequency (using one of FreqDist's built in methods)\n",
    "\tsorted_tag_counts = tag_counts.most_common()\n",
    "\t\n",
    "     # print out each tag and the percentage of tokens associated with it, in descending order \n",
    "\tfor item in sorted_tag_counts:\n",
    "\t\ttag_percent = 100 * item[1]/n\n",
    "\t\tp = '{0:.2f}'.format(tag_percent)\n",
    "\t\tprint('Tag:',item[0],'\\t   Percentage of tokens = ', p )\n",
    "  \n",
    "\treturn( tokens_and_tags  )\n",
    "\t\n",
    "\t\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# PROBLEM 3\n",
    "# Complete the definition of the function below so that it \n",
    "# - reads input from the specified filename, e.g., yelp_reviews.json (using json.load)\n",
    "# - extracts the text of the kth review\n",
    "# - runs the parts_of_speech function (Problem 2) to compute the percentages of tokens for each part of speech\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def review_pos(k,filename): \n",
    "\t \n",
    "\tprint('\\nLoading the file: \\n', filename) \n",
    "\twith open(filename, 'r') as jfile:\n",
    "\t\tdata = jsoncm.load(jfile)\n",
    "\tprint('\\nTotal number of reviews extracted =', len(data))\n",
    " \n",
    "\tprint('\\nComputing the percentages for each part-of-speech for review',k)\n",
    "\t\n",
    "\td = data[k-1]  # extract the kth review (indexed from 0)\n",
    "\ts = d['text']  # extract text string associated with kth review\n",
    "\tprint('Text from review ',k, ' is:')\n",
    "\tprint(s)\n",
    "\tparts_of_speech(s)\n",
    "\t\t\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# PROBLEM 4\n",
    "# Create a bag of words (BOW) representation from text documents, using the Vectorizer function in scikit-learn\n",
    "#\n",
    "# The inputs are \n",
    "#  - a filename (you will use \"yelp_reviews.json\") containing the reviews in JSON format \n",
    "#  - the min_pos and max_neg parameters\n",
    "#  - we label all reviews with scores > min_pos = 4 as \"1\"  \n",
    "#  - we label all reviews with scores < max_neg = 2 as \"0\" \n",
    "#  - this creates a simple set of labels for binary classification, ignoring the neutral (score = 3) reviews\n",
    "# \n",
    "#  The function extracts the text and scores for each review from the JSON data\n",
    "#  It then tokenizes and creates a sparse bag-of-words array using scikit-learn vectorizer function\n",
    "#  The number of rows in the array is the number of reviews with scores <=2 or >=4\n",
    "#  The number of columns in the array is the number of terms in the vocabulary\n",
    "#\n",
    "#  NOTE: \n",
    "#  - please read the scikit-learn tutorial on text feature extraction before you start this problem:\n",
    "#     https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "#  - in this function we will use scikit-learn tokenization (rather than NLTK)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "def create_bow_from_reviews(filename, min_pos=4, max_neg=2): \n",
    "\t\n",
    "\tprint('\\nLoading the file: \\n', filename) \n",
    "\twith open(filename, 'r') as jfile:\n",
    "\t\tdata = jsoncm.load(jfile)\n",
    "\tprint('\\nTotal number of reviews extracted =', len(data) )\n",
    "\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tlengths = []\n",
    "\tprint('\\nExtracting tokens from each review.....(can be slow for a large number of reviews)......')   \n",
    "\tfor d in data: \t# can substitute data[0:9] here if you want to test this function on just a few reviews\n",
    "\t\treview = d['text']    # keep only the text and label\n",
    "\t\tstars = d['stars']\n",
    "        #....write some simple logic to generate a binary score for each review\n",
    "\t\tif stars >= min_pos:\n",
    "\t\t\tscore = 1\n",
    "\t\t\ttext.append(review)\n",
    "\t\t\tY.append(score)\n",
    "\t\telif stars <= max_neg:\n",
    "\t\t\tscore = 0\n",
    "\t\t\ttext.append(review)\n",
    "\t\t\tY.append(score)\n",
    "\n",
    "    \n",
    "    # create an instance of a CountVectorizer, using \n",
    "    # (1) the standard 'english' stopword set \n",
    "    # (2) only keeping terms in the vocabulary that occur in at least 1% of documents\n",
    "    # (3) allowing both unigrams and bigrams in the vocabulary (use \"ngram_range=(1,2)\" to do this)\n",
    "\tvectorizer = CountVectorizer(stop_words='english',min_df=0.01,ngram_range=(1,2))\n",
    "\t\n",
    "\t# create a sparse BOW array from 'text' using vectorizer  \n",
    "\tX = vectorizer.fit_transform(text)\n",
    "\t\n",
    "\t# an alternative above would be to use TfIDF rather than counts - which is very simple to do (but not needed here)\n",
    " \n",
    "\tprint('Data shape: ', X.shape)\n",
    "\t\n",
    "\t# you can uncomment this next line if you want to see the full list of tokens in the vocabulary  \n",
    "\t#print('Vocabulary: ', vectorizer.get_feature_names())\n",
    " \n",
    "\treturn X, Y, vectorizer\n",
    "\t\t \n",
    "\t\t \n",
    "\t\t \n",
    "# ---------------------------------------------------------------------------------------\n",
    "# PROBLEM 5\n",
    "#  Separate an X,Y dataset (X=features, Y=labels) into training and test subsets\n",
    "#  Build a logistic classifier on the training subset\n",
    "#  Evaluate performance on the test subset  \n",
    "#\n",
    "#  NOTE: before starting this problem please read the scikit-learn documentation on logistic classifiers:\n",
    "#\t\thttps://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "# ---------------------------------------------------------------------------------------\t\t \n",
    "def logistic_classification(X, Y, test_fraction):\n",
    "    # should add comments defining what the inputs are what the function does\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_fraction, random_state=42)\n",
    "    #  set the state of the random number generator so that we get the same results across runs when testing our code\n",
    "\n",
    "    print('Number of training examples: ', X_train.shape[0])\n",
    "    print('Number of testing examples: ', X_test.shape[0])\n",
    "    print('Vocabulary size: ', X_train.shape[1])\n",
    "\n",
    "    # Specify the logistic classifier model with an l2 penalty for regularization and with fit_intercept turned on\n",
    "    classifier = linear_model.LogisticRegression(penalty='l2', fit_intercept=True)\n",
    "\n",
    "    # Train a logistic regression classifier and evaluate accuracy on the training data\n",
    "    print('\\nTraining a model with', X_train.shape[0], 'examples.....')\n",
    "    classifier.fit(X_train, Y_train)\n",
    "    train_predictions = classifier.predict(X_train)\n",
    "    train_accuracy = classifier.score(X_train, Y_train)\n",
    "    print('\\nTraining:')\n",
    "    print(' accuracy:', format(100 * train_accuracy, '.2f'))\n",
    "\n",
    "    # Compute and print accuracy and AUC on the test data\n",
    "    print('\\nTesting: ')\n",
    "    test_predictions = classifier.predict(X_test)\n",
    "    test_accuracy = classifier.score(X_test, Y_test)\n",
    "    print(' accuracy:', format(100 * test_accuracy, '.2f'))\n",
    "\n",
    "    class_probabilities = classifier.predict_proba(X_test)\n",
    "    test_auc_score = metrics.roc_auc_score(Y_test, class_probabilities[:, 1])\n",
    "    print(' AUC value:', format(100 * test_auc_score, '.2f'))\n",
    "\n",
    "    return (classifier)\n",
    "    \n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# PROBLEM 6\n",
    "#   Takes as input\n",
    "#     (1) a scikit-learn trained logistic regression classifier (e.g., trained in Problem 5) \n",
    "#     (2) a scikit-learn vectorizer object that produced the BOW features for the classifier\n",
    "#   and prints out and returns\n",
    "#   - the K terms in the vocabulary tokens with the largest positive weights  \n",
    "#   - the K terms in the vocabulary with the largest negative weights \n",
    "#\n",
    "# To write this code you will need to read the documentation for the logistic regression model \n",
    "# in scikit-learn to figure out how to access the classifier's weights and corresponding tokens\n",
    "# ---------------------------------------------------------------------------------------\t\t\t\t\n",
    "\n",
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "\tterms = vectorizer.get_feature_names() #term\n",
    "\tweight = classifier.coef_[0] #weight\n",
    "\tpair = list(zip(terms,weight))\n",
    "\tpair.sort(key=lambda x:x[1],reverse=True)\n",
    "\n",
    "     # You can write this function in whatever way you want\n",
    "     \n",
    "     # ...after you find the relevant weights and terms....     \n",
    "     # ....cycle through the positive weights, in the order of largest weight first and print out\n",
    "     # K lines where each line contains \n",
    "     # (a) the term corresponding to the weight (a string)\n",
    "     # (b) the weight value itself (a scalar printed to 3 decimal places)\n",
    "\ttopK_pos_weights = [i[1] for i in pair[:K]]\n",
    "\ttopK_pos_terms = [i[0] for i in pair[:K]]\n",
    "\ttopK_neg_weights = [i[1] for i in pair[-K:]]\n",
    "\ttopK_neg_terms = [i[0] for i in pair[-K:]]\n",
    "\ttopK_neg_terms.reverse()\n",
    "\ttopK_neg_weights.reverse()\n",
    "# e.g., for w in topK_pos_weights:\n",
    "#      \t...\n",
    "#      \tprint( ....\n",
    "#      \t\n",
    "     # Same for negative weights, most negative values first\n",
    "#      for w in topK_neg_weights:\n",
    "#      \t...\n",
    "#      \tprint( ....\n",
    "\tprint(\"top \",K,\" words token withe the positive weight:\")\n",
    "\tfor i in range(len(topK_pos_weights)):\n",
    "\t\tprint(topK_pos_terms[i] ,\" : \", topK_pos_weights[i])\n",
    "\tprint(\"\\ntop \",K,\" words token withe the positive weight:\")\n",
    "\tfor i in range(len(topK_neg_weights)):\n",
    "\t\tprint(topK_neg_terms[i], \" : \", topK_neg_weights[i])\n",
    "\n",
    "\n",
    "\n",
    "\treturn(topK_pos_weights, topK_neg_weights, topK_pos_terms, topK_neg_terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage =  10.34  of characters match the character t\n",
      "Total number of tokens is 17\n",
      "Tag: NOUN \t   Percentage of tokens =  35.29\n",
      "Tag: DET \t   Percentage of tokens =  17.65\n",
      "Tag: VERB \t   Percentage of tokens =  11.76\n",
      "Tag: ADP \t   Percentage of tokens =  11.76\n",
      "Tag: ADV \t   Percentage of tokens =  5.88\n",
      "Tag: ADJ \t   Percentage of tokens =  5.88\n",
      "Tag: PRT \t   Percentage of tokens =  5.88\n",
      "Tag: . \t   Percentage of tokens =  5.88\n",
      "\n",
      "Loading the file: \n",
      " yelp_reviews.json\n",
      "\n",
      "Total number of reviews extracted = 20000\n",
      "\n",
      "Computing the percentages for each part-of-speech for review 2\n",
      "Text from review  2  is:\n",
      "If you need an inexpensive place to stay for a night or two then you may consider this place but for a longer stay I'd recommend somewhere with better amenities. \n",
      "\n",
      "Pros:\n",
      "Great location- you're right by the train station, central location to get to old town and new town, and right by sight seeing his tours. Food, bars, and shopping all within walking distance. Location, location, location.\n",
      "Very clean and very good maid service\n",
      "\n",
      "Cons:\n",
      "Tiny rooms \n",
      "Uncomfortable bed \n",
      "Absolutely no amenities \n",
      "No phone in room \n",
      "No wardrobe \n",
      "\n",
      "Was given a lot of attitude about me and my husband sharing a room which was quite strange and we were charged 15 pounds more for double occupancy not sure why that matters I felt like it was a money grab. It was just handled in a kind of odd manner to me... \n",
      "\n",
      "If you book this hotel all you get is a bed, desk, and a bathroom. It isn't awful but know what you're getting into.\n",
      "Total number of tokens is 187\n",
      "Tag: NOUN \t   Percentage of tokens =  25.67\n",
      "Tag: VERB \t   Percentage of tokens =  14.44\n",
      "Tag: . \t   Percentage of tokens =  9.63\n",
      "Tag: PRON \t   Percentage of tokens =  9.09\n",
      "Tag: DET \t   Percentage of tokens =  9.09\n",
      "Tag: ADP \t   Percentage of tokens =  8.56\n",
      "Tag: ADJ \t   Percentage of tokens =  8.56\n",
      "Tag: ADV \t   Percentage of tokens =  6.42\n",
      "Tag: CONJ \t   Percentage of tokens =  5.35\n",
      "Tag: PRT \t   Percentage of tokens =  2.14\n",
      "Tag: NUM \t   Percentage of tokens =  1.07\n",
      "\n",
      "Loading the file: \n",
      " yelp_reviews.json\n",
      "\n",
      "Total number of reviews extracted = 20000\n",
      "\n",
      "Extracting tokens from each review.....(can be slow for a large number of reviews)......\n",
      "Data shape:  (17501, 849)\n",
      "Number of training examples:  3500\n",
      "Number of testing examples:  14001\n",
      "Vocabulary size:  849\n",
      "\n",
      "Training a model with 3500 examples.....\n",
      "\n",
      "Training:\n",
      " accuracy: 97.89\n",
      "\n",
      "Testing: \n",
      " accuracy: 89.33\n",
      " AUC value: 92.87\n",
      "top  10  words token withe the positive weight:\n",
      "amazing  :  2.114320916794638\n",
      "awesome  :  2.0337550111611553\n",
      "delicious  :  1.8554659278672017\n",
      "excellent  :  1.8489143944834998\n",
      "friendly  :  1.691340223125593\n",
      "great  :  1.5373060421314768\n",
      "fantastic  :  1.5139529722362448\n",
      "fast  :  1.5118529862421104\n",
      "definitely  :  1.447075356955066\n",
      "world  :  1.3667725492218181\n",
      "\n",
      "top  10  words token withe the positive weight:\n",
      "worst  :  -2.469812488279568\n",
      "horrible  :  -2.090375478490818\n",
      "maybe  :  -1.8383668628682437\n",
      "dirty  :  -1.7899077954520253\n",
      "ok  :  -1.7780969145833994\n",
      "terrible  :  -1.617362442432416\n",
      "bland  :  -1.5325835676573403\n",
      "slow  :  -1.4893455503955593\n",
      "rude  :  -1.4307321070268648\n",
      "stay  :  -1.3917856100701127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.114320916794638,\n",
       "  2.0337550111611553,\n",
       "  1.8554659278672017,\n",
       "  1.8489143944834998,\n",
       "  1.691340223125593,\n",
       "  1.5373060421314768,\n",
       "  1.5139529722362448,\n",
       "  1.5118529862421104,\n",
       "  1.447075356955066,\n",
       "  1.3667725492218181],\n",
       " [-2.469812488279568,\n",
       "  -2.090375478490818,\n",
       "  -1.8383668628682437,\n",
       "  -1.7899077954520253,\n",
       "  -1.7780969145833994,\n",
       "  -1.617362442432416,\n",
       "  -1.5325835676573403,\n",
       "  -1.4893455503955593,\n",
       "  -1.4307321070268648,\n",
       "  -1.3917856100701127],\n",
       " ['amazing',\n",
       "  'awesome',\n",
       "  'delicious',\n",
       "  'excellent',\n",
       "  'friendly',\n",
       "  'great',\n",
       "  'fantastic',\n",
       "  'fast',\n",
       "  'definitely',\n",
       "  'world'],\n",
       " ['worst',\n",
       "  'horrible',\n",
       "  'maybe',\n",
       "  'dirty',\n",
       "  'ok',\n",
       "  'terrible',\n",
       "  'bland',\n",
       "  'slow',\n",
       "  'rude',\n",
       "  'stay'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_percentage('This is a sentence about a large dog.','t') \n",
    "# test the parts_of_speech function with simple input\n",
    "parts_of_speech('This is a very simple test sentence to test the part of speech function in NLTK.')\n",
    "\n",
    "\n",
    "# load yelp reviews and compute percentages of parts of speech for the Kth review\n",
    "K = 2\n",
    "review_pos(K,'yelp_reviews.json') \n",
    "\n",
    "# read in the review text and tokenize the text in each review\n",
    "X, Y , vectorizer_BOW = create_bow_from_reviews('yelp_reviews.json')   \n",
    "\n",
    "\n",
    "# run a logistic classifier on the reviews, specifying the fraction to be used for testing  \n",
    "test_fraction = 0.8\n",
    "logistic_classifier = logistic_classification(X, Y,test_fraction)  \n",
    "\n",
    "\n",
    "# print out and return the most significant positive and negative weights (and associated terms) \n",
    "most_significant_terms(logistic_classifier, vectorizer_BOW, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
